{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa5c3f8c",
   "metadata": {},
   "source": [
    "# 1 - Introduction\n",
    "\n",
    "## Domain-specific area / Problem space\n",
    "### Definition of Fake News\n",
    " Fake news is a phrase we hear more and more recently, but what exactly is fake news? Simply put fake news is news that is inaccurate, often on purpose, and spreads disinformation and misinformation for the purpose to gain attention, mislead, deceive or even sway people's voting behavior(https://en.wikipedia.org/wiki/Fake_news).\n",
    "### Examples of real world consequences\n",
    " This, although seemingly just an annoyance, can have many real world consequences. An example of these are the reactions to what is known as \"Pizza Gate\". This was a fake news turned conspiracy theory that spread online that spread the idea that certain pizza chains had a basement where children were being held for selling as sexual object. One man became convinced it was true, and showed up at one such restaurant, armed and threatened violence and demanded the staff show him the basement so he could rescue the children(https://www.washingtonpost.com/news/local/wp/2016/12/04/d-c-police-respond-to-report-of-a-man-with-a-gun-at-comet-ping-pong-restaurant/)\n",
    "### Area of of NLP contribution\n",
    " Journalism websites that value fact checked quality news, would benefit from such a detector.This problem is essentially one of text classification, a subset of natural language processing, where text is categorized into groups and sub groups. As such, a fake news detector can help automate the task of filtering out fake news, much as a spam filter filters out spam email, to help prevent any news or media outlet from accidentally spreading misinformation.  \n",
    "## Objectives\n",
    "I for one am tired of getting my hopes up that Bigfoot has been found singing karaoke, only to have my hopes and dreams dashed after closer scrutiny.\n",
    "Therefore, in this notebook, I will attempt to create a classifier that (hopefully) with a relatively high degree of accuracy can detect if a given text is fake or real news. Using existing libraries from python I will train a model to sniff out fake news. If succesfull, hopefully this project could help contribute to preventing the spread of false information and help the news outlets maintain a high quality of journalism. \n",
    "### Chosen methods and justification\n",
    "Being that I would like my detector to read the entirety of the text, I have decided to use Term Frequency Inverse Document Frequency (Tfids) in order to determine the relative importance of text. Since this method focuses on how important a given word or set of words is to a corpus and information retrieval as well as frequency distrobution, my hope is that it can find commonalities in the text types used in fake news versus real news. It also handily removes stop words which helps simplify oour cleaning of the data. Put simply, this looks at the frequency of words in a corpus, as well as the relative weight or importance of the words in the document.\n",
    "### Classifiers\n",
    "For this I want to try two differnet classifers. The first being the one we learned in class, the Naive Bayes MultinomialNB. I am intersted to see how a relativley simple naive bayes based model performs in classifying something as nebulous and difficult as fake news.\n",
    "As a second classifer, I found after some digging around on the subject of fake news detecotrs and classifiers in the following paper: \n",
    "- ( Gupta, S., Meel, P. (2021). Fake News Detection Using Passive-Aggressive Classifier. In: Ranganathan, G., Chen, J., Rocha, Á. (eds) Inventive Communication and Computational Technologies. Lecture Notes in Networks and Systems, vol 145. Springer, Singapore. https://doi.org/10.1007/978-981-15-7345-3_13 )\n",
    "So of course I want to test it against naive bayes and see how they compare.\n",
    "\n",
    "## Dataset \n",
    "The dataset acquired for this project was procured from Kaggle as a series of csv files. There are three, a train.csv file which contains various real and fake news articles, with label of 0 for a \"Real\" article and 1 for a \"Fake\" article. The model will be trained and tested on this. There are also two others labeled Real.csv and Fake.csv. This are two files that contain only Real and Fake news articles respectively. I will use those to create a test of my own for accuracy (i.e. I will see how many of the fake news articles my detector actually thinks are fake )\n",
    "### Dataset characteristics\n",
    "The training dataset is divided into the follwing Categories represented in each column:\n",
    "- id: the numerical id of the row\n",
    "- title: the title of the article\n",
    "- author: the author of the article\n",
    "- text: the body of the article itself\n",
    "- label: The label of Real or Fake it is classified as, 0 for Real news, 1 for Fake\n",
    "\n",
    "train_csv found here: https://www.kaggle.com/competitions/fake-news/data?select=train.csv\n",
    "\n",
    "There is also a Fake.csv  data files sourced from a differnet set that I will use to perform a \"field test\" of the data. This data set has the following Categoires in each column:\n",
    "\n",
    "- title: The title of the artice\n",
    "- text: The bodt of text of the news article\n",
    "- subject: The topic of the news report\n",
    "- date: The date it was published\n",
    "\n",
    "Fake.csv data can be found here: https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset?select=True.csv\n",
    "\n",
    "\n",
    "## Evaluation methods \n",
    "### Method 1 - accuracy using python library\n",
    "Given that the intended use of this is to filter out fake news articles, then the number of actual fake news articles it detects versus how many there were would be the most important thing. Therefore, I will judge the success of the model based on its accuracy score. I will check its accuracy in the following two ways. First, I will use the python libraries metrics library to see the easily readable report (classification_report) that will use the testing data from the csv file train_csv. \n",
    "### Method 2 -  A \"field test\" with the trained model\n",
    "As a secondary check, I will create my own function using the model we trained that will check individually the fake news csv file text and divide that number by the total text. The idea being, it should recognize all of them as \"fake\", but if it does not, then I can see what percentage of it it was able to recognize ( as a kind of real world test )."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ac1472",
   "metadata": {},
   "source": [
    "# 2 - Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154ead29",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2318c8",
   "metadata": {},
   "source": [
    "### Getting the csv data as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f764e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas and assign dataframes with the csv data\n",
    "import pandas as pd\n",
    "df_fake = pd.read_csv(\"Fake.csv\") # the 'field test' data\n",
    "df_train = pd.read_csv(\"train.csv\") # the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7641a597",
   "metadata": {},
   "source": [
    "### Checking the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10abceb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20795</th>\n",
       "      <td>20795</td>\n",
       "      <td>Rapper T.I.: Trump a ’Poster Child For White S...</td>\n",
       "      <td>Jerome Hudson</td>\n",
       "      <td>Rapper T. I. unloaded on black celebrities who...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20796</th>\n",
       "      <td>20796</td>\n",
       "      <td>N.F.L. Playoffs: Schedule, Matchups and Odds -...</td>\n",
       "      <td>Benjamin Hoffman</td>\n",
       "      <td>When the Green Bay Packers lost to the Washing...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20797</th>\n",
       "      <td>20797</td>\n",
       "      <td>Macy’s Is Said to Receive Takeover Approach by...</td>\n",
       "      <td>Michael J. de la Merced and Rachel Abrams</td>\n",
       "      <td>The Macy’s of today grew from the union of sev...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20798</th>\n",
       "      <td>20798</td>\n",
       "      <td>NATO, Russia To Hold Parallel Exercises In Bal...</td>\n",
       "      <td>Alex Ansary</td>\n",
       "      <td>NATO, Russia To Hold Parallel Exercises In Bal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20799</th>\n",
       "      <td>20799</td>\n",
       "      <td>What Keeps the F-35 Alive</td>\n",
       "      <td>David Swanson</td>\n",
       "      <td>David Swanson is an author, activist, journa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20800 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  \\\n",
       "0          0  House Dem Aide: We Didn’t Even See Comey’s Let...   \n",
       "1          1  FLYNN: Hillary Clinton, Big Woman on Campus - ...   \n",
       "2          2                  Why the Truth Might Get You Fired   \n",
       "3          3  15 Civilians Killed In Single US Airstrike Hav...   \n",
       "4          4  Iranian woman jailed for fictional unpublished...   \n",
       "...      ...                                                ...   \n",
       "20795  20795  Rapper T.I.: Trump a ’Poster Child For White S...   \n",
       "20796  20796  N.F.L. Playoffs: Schedule, Matchups and Odds -...   \n",
       "20797  20797  Macy’s Is Said to Receive Takeover Approach by...   \n",
       "20798  20798  NATO, Russia To Hold Parallel Exercises In Bal...   \n",
       "20799  20799                          What Keeps the F-35 Alive   \n",
       "\n",
       "                                          author  \\\n",
       "0                                  Darrell Lucus   \n",
       "1                                Daniel J. Flynn   \n",
       "2                             Consortiumnews.com   \n",
       "3                                Jessica Purkiss   \n",
       "4                                 Howard Portnoy   \n",
       "...                                          ...   \n",
       "20795                              Jerome Hudson   \n",
       "20796                           Benjamin Hoffman   \n",
       "20797  Michael J. de la Merced and Rachel Abrams   \n",
       "20798                                Alex Ansary   \n",
       "20799                              David Swanson   \n",
       "\n",
       "                                                    text  label  \n",
       "0      House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1      Ever get the feeling your life circles the rou...      0  \n",
       "2      Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3      Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4      Print \\nAn Iranian woman has been sentenced to...      1  \n",
       "...                                                  ...    ...  \n",
       "20795  Rapper T. I. unloaded on black celebrities who...      0  \n",
       "20796  When the Green Bay Packers lost to the Washing...      0  \n",
       "20797  The Macy’s of today grew from the union of sev...      0  \n",
       "20798  NATO, Russia To Hold Parallel Exercises In Bal...      1  \n",
       "20799    David Swanson is an author, activist, journa...      1  \n",
       "\n",
       "[20800 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the dataset\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0506b318",
   "metadata": {},
   "source": [
    "### Changing the labels\n",
    "I find the 0 for Real and 1 for fake to be a bit hard to read, so here I am going to make a dict to replace all of them with the lables \"Real\" and \"Fake\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1235170b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20795</th>\n",
       "      <td>20795</td>\n",
       "      <td>Rapper T.I.: Trump a ’Poster Child For White S...</td>\n",
       "      <td>Jerome Hudson</td>\n",
       "      <td>Rapper T. I. unloaded on black celebrities who...</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20796</th>\n",
       "      <td>20796</td>\n",
       "      <td>N.F.L. Playoffs: Schedule, Matchups and Odds -...</td>\n",
       "      <td>Benjamin Hoffman</td>\n",
       "      <td>When the Green Bay Packers lost to the Washing...</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20797</th>\n",
       "      <td>20797</td>\n",
       "      <td>Macy’s Is Said to Receive Takeover Approach by...</td>\n",
       "      <td>Michael J. de la Merced and Rachel Abrams</td>\n",
       "      <td>The Macy’s of today grew from the union of sev...</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20798</th>\n",
       "      <td>20798</td>\n",
       "      <td>NATO, Russia To Hold Parallel Exercises In Bal...</td>\n",
       "      <td>Alex Ansary</td>\n",
       "      <td>NATO, Russia To Hold Parallel Exercises In Bal...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20799</th>\n",
       "      <td>20799</td>\n",
       "      <td>What Keeps the F-35 Alive</td>\n",
       "      <td>David Swanson</td>\n",
       "      <td>David Swanson is an author, activist, journa...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20800 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  \\\n",
       "0          0  House Dem Aide: We Didn’t Even See Comey’s Let...   \n",
       "1          1  FLYNN: Hillary Clinton, Big Woman on Campus - ...   \n",
       "2          2                  Why the Truth Might Get You Fired   \n",
       "3          3  15 Civilians Killed In Single US Airstrike Hav...   \n",
       "4          4  Iranian woman jailed for fictional unpublished...   \n",
       "...      ...                                                ...   \n",
       "20795  20795  Rapper T.I.: Trump a ’Poster Child For White S...   \n",
       "20796  20796  N.F.L. Playoffs: Schedule, Matchups and Odds -...   \n",
       "20797  20797  Macy’s Is Said to Receive Takeover Approach by...   \n",
       "20798  20798  NATO, Russia To Hold Parallel Exercises In Bal...   \n",
       "20799  20799                          What Keeps the F-35 Alive   \n",
       "\n",
       "                                          author  \\\n",
       "0                                  Darrell Lucus   \n",
       "1                                Daniel J. Flynn   \n",
       "2                             Consortiumnews.com   \n",
       "3                                Jessica Purkiss   \n",
       "4                                 Howard Portnoy   \n",
       "...                                          ...   \n",
       "20795                              Jerome Hudson   \n",
       "20796                           Benjamin Hoffman   \n",
       "20797  Michael J. de la Merced and Rachel Abrams   \n",
       "20798                                Alex Ansary   \n",
       "20799                              David Swanson   \n",
       "\n",
       "                                                    text label  \n",
       "0      House Dem Aide: We Didn’t Even See Comey’s Let...  Fake  \n",
       "1      Ever get the feeling your life circles the rou...  Real  \n",
       "2      Why the Truth Might Get You Fired October 29, ...  Fake  \n",
       "3      Videos 15 Civilians Killed In Single US Airstr...  Fake  \n",
       "4      Print \\nAn Iranian woman has been sentenced to...  Fake  \n",
       "...                                                  ...   ...  \n",
       "20795  Rapper T. I. unloaded on black celebrities who...  Real  \n",
       "20796  When the Green Bay Packers lost to the Washing...  Real  \n",
       "20797  The Macy’s of today grew from the union of sev...  Real  \n",
       "20798  NATO, Russia To Hold Parallel Exercises In Bal...  Fake  \n",
       "20799    David Swanson is an author, activist, journa...  Fake  \n",
       "\n",
       "[20800 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A dictionary to with 0 as the Real and 1 as Fale\n",
    "conversion_dict = {0:\"Real\", 1:\"Fake\"}\n",
    "# Replace all labels that correspond to 0 or 1 as Real and Fake\n",
    "df_train['label'] = df_train['label'].replace(conversion_dict)\n",
    "# Check we did it\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e003a2a5",
   "metadata": {},
   "source": [
    "### Cleaning the data\n",
    "The data looks fairly clean already, but just to be sure lets check if there are any duplicate data or empty rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b365a315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20800, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for duplicates and see if we still have 20,800 rows\n",
    "df_train.drop_duplicates(inplace=True)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f51e709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           0\n",
       "title      558\n",
       "author    1957\n",
       "text        39\n",
       "label        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see how many, if any, columns have empty rows\n",
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "382e6aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18285, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop the empties\n",
    "df_train.dropna(axis=0, inplace=True)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf302c9e",
   "metadata": {},
   "source": [
    "Good thing we checked, that is nearly 2,000 empty rows. Might have thrown our model off. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad17417",
   "metadata": {},
   "source": [
    "## Benchmarks\n",
    "\n",
    "According to this[1] paper on fake news detection, they achieved an accuracy of 74% with Naive Bayes.\n",
    "\n",
    "The accuracy achived with a Passive Agressive Calssifier was reported at 90.8% according to this research[2] ( and using the same dataset I trained on )\n",
    "\n",
    "- Naive Bayes Paper- Sharma, D.K., Garg, S. IFND: a benchmark dataset for fake news detection. Complex Intell. Syst. (2021). https://doi.org/10.1007/s40747-021-00552-1\n",
    "\n",
    "- Passive Agessive Classifier Paper - B.Suganthi, K.Manohari, FAKEDETECTOR: Effective Fake News Detection with Passive Aggressive Algorithm Science, Technology and Development Volume X Issue IX SEPTEMBER 2021 ISSN : 0950-0707 (2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a1e531",
   "metadata": {},
   "source": [
    "## Spliting the data and applying tfid for tokenzing\n",
    "Now for the fun, let's seperate our data into training groups and apply tfid to it. First we will need to download some libraies and install scikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1109b686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /opt/anaconda3/envs/nlp/lib/python3.8/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/nlp/lib/python3.8/site-packages (from sklearn) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/envs/nlp/lib/python3.8/site-packages (from scikit-learn->sklearn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/anaconda3/envs/nlp/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.21.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/anaconda3/envs/nlp/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.8.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /opt/anaconda3/envs/nlp/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "# Installing sklearn\n",
    "import sys\n",
    "!{sys.executable} -m pip install sklearn\n",
    "\n",
    "# Getting a test training split and feature extraction modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd33e0e4",
   "metadata": {},
   "source": [
    "## Removing stop words, ignoring overly abbundant commonalities and spliting into testing and training groups\n",
    "Now lets split into training and testing. I'm going with 20% for testing, which gives us 80% for training. We are also shuffling the data and adding a random state to prevent any unbalnaced splits.\n",
    "The TfidfVectorizer conveniently has a parameter for stop words, so we will go ahead and fill that in, and give it a max_df of 0.7 ( meanind to ignore text that is common in 70% or more of the text ). \n",
    "## Term frequency and inverse document frequncy\n",
    "The most important aspect as to why I chose the tfidf for tokenzing the words is due to the fact that not only does it categorize the wrods by frequency, but also by the inverse document frequcy, or that is to say by how \"important\" they seem to be for the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5f700df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training and testing data. \n",
    "x_train, x_test, y_train, y_test = train_test_split(df_train['text'], df_train['label'], test_size=0.2, random_state=7, shuffle=True)\n",
    "# Set the vectorizer, and removing english stop words. max_df removes any text that\n",
    "#  correlates to 70% and above of the text\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59bbdb6",
   "metadata": {},
   "source": [
    "## Tokenizing our data \n",
    "Now to seperate our training and testing and fit to add to our classifier. I found that makepipeline doesn't play well with pandas dataframes ( or at least I couldn't get it to ) so we will be doing this the long way. Notice we had to assign the type as 'U' for unicode in order for everything to be ready for our vecotrizer and classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6be14a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vectors/tokens and fit for our model\n",
    "tfidf_vec_train = tfidf_vectorizer.fit_transform(x_train.values.astype('U')) # assign type U for unicode\n",
    "#set aside the tokens for testing oour model\n",
    "tfidf_vec_test = tfidf_vectorizer.transform(x_test.values.astype('U'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42e15c6",
   "metadata": {},
   "source": [
    "## Choosing and comparing our classifiers\n",
    "For this I have decided to compare two different classifiers and see how they perform. This is because through the course of the module, I was taught about one, the naive bayes Multinomial for text classification, so I thought it would work well for this project. However, I also want to compare with a different calssifier known as the PassiveAgressive Calssifier. During my research into building a Fake news Detector, I found some articles on this classifier, and wondered if it would be a better choice or not.\n",
    "\n",
    "### Selected features\n",
    "The features I deicided to go with was the body of the text itself, and using the Tfidf vecotrizor I removed stop words as well as any words that were common in 70% or more of the text. In this way I think I can avoid commonalities between the texts and focus on the words that tend to be more prevalent in fake news. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cebcd31",
   "metadata": {},
   "source": [
    "### MultinomialNB \n",
    "The Multinomial Naive Bayes algorithm is a Bayesian learning technique where it guesses the tag of a text, in this case \"Real\" or \"Fake\", using the Bayes theorem. It calculates each tag's likelihood for a given sample and outputs the tag with the greatest chance. Let's see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b1d074d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the Naive Bayes classifer and store it\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "NB_classifier = MultinomialNB()\n",
    "NB_classifier.fit(tfidf_vec_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d10dfa2",
   "metadata": {},
   "source": [
    "## Performace of the MultinomialNB\n",
    "Lets check our performance and see how well we are doing.\n",
    "First we will need to download the libaries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0acdbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the accuracy score module\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60ffa49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Accuracy: 77.96%\n"
     ]
    }
   ],
   "source": [
    "# Set up our prediciton, score and calcualte the accuracy, rounded to two decimal places\n",
    "nb_y_pred = NB_classifier.predict(tfidf_vec_test)\n",
    "nb_score = accuracy_score(y_test, nb_y_pred)\n",
    "MNB_accuracy = round(nb_score*100,2)\n",
    "print(f'NB Accuracy: {MNB_accuracy}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4508c869",
   "metadata": {},
   "source": [
    "77.96%, not too shabby considering our benchmark of 74%! Let's take a closer look at how we did."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34373426",
   "metadata": {},
   "source": [
    "## Get the classifcation report and see the detailed results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b191efa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.99      0.49      0.66      1580\n",
      "        Real       0.72      1.00      0.84      2077\n",
      "\n",
      "    accuracy                           0.78      3657\n",
      "   macro avg       0.86      0.75      0.75      3657\n",
      "weighted avg       0.84      0.78      0.76      3657\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See the detialed report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, nb_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095f3037",
   "metadata": {},
   "source": [
    "Well accuracy is what we are most concered with, but intersting the differnece in precision between real and fake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb685e1",
   "metadata": {},
   "source": [
    "## Checking with custom algorithm, a.k.a our \"field test\"\n",
    "Here I wanted to see how well it did when given purely Fake news examples that I have prepared above. I will run through each text in both files, and calculate the percentage of them it correctly tagged as \"Real\" or \"Fake.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf75cfe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23476</th>\n",
       "      <td>McPain: John McCain Furious That Iran Treated ...</td>\n",
       "      <td>21st Century Wire says As 21WIRE reported earl...</td>\n",
       "      <td>Middle-east</td>\n",
       "      <td>January 16, 2016</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23477</th>\n",
       "      <td>JUSTICE? Yahoo Settles E-mail Privacy Class-ac...</td>\n",
       "      <td>21st Century Wire says It s a familiar theme. ...</td>\n",
       "      <td>Middle-east</td>\n",
       "      <td>January 16, 2016</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23478</th>\n",
       "      <td>Sunnistan: US and Allied ‘Safe Zone’ Plan to T...</td>\n",
       "      <td>Patrick Henningsen  21st Century WireRemember ...</td>\n",
       "      <td>Middle-east</td>\n",
       "      <td>January 15, 2016</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23479</th>\n",
       "      <td>How to Blow $700 Million: Al Jazeera America F...</td>\n",
       "      <td>21st Century Wire says Al Jazeera America will...</td>\n",
       "      <td>Middle-east</td>\n",
       "      <td>January 14, 2016</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23480</th>\n",
       "      <td>10 U.S. Navy Sailors Held by Iranian Military ...</td>\n",
       "      <td>21st Century Wire says As 21WIRE predicted in ...</td>\n",
       "      <td>Middle-east</td>\n",
       "      <td>January 12, 2016</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23481 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "0       Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1       Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2       Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3       Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4       Pope Francis Just Called Out Donald Trump Dur...   \n",
       "...                                                  ...   \n",
       "23476  McPain: John McCain Furious That Iran Treated ...   \n",
       "23477  JUSTICE? Yahoo Settles E-mail Privacy Class-ac...   \n",
       "23478  Sunnistan: US and Allied ‘Safe Zone’ Plan to T...   \n",
       "23479  How to Blow $700 Million: Al Jazeera America F...   \n",
       "23480  10 U.S. Navy Sailors Held by Iranian Military ...   \n",
       "\n",
       "                                                    text      subject  \\\n",
       "0      Donald Trump just couldn t wish all Americans ...         News   \n",
       "1      House Intelligence Committee Chairman Devin Nu...         News   \n",
       "2      On Friday, it was revealed that former Milwauk...         News   \n",
       "3      On Christmas day, Donald Trump announced that ...         News   \n",
       "4      Pope Francis used his annual Christmas Day mes...         News   \n",
       "...                                                  ...          ...   \n",
       "23476  21st Century Wire says As 21WIRE reported earl...  Middle-east   \n",
       "23477  21st Century Wire says It s a familiar theme. ...  Middle-east   \n",
       "23478  Patrick Henningsen  21st Century WireRemember ...  Middle-east   \n",
       "23479  21st Century Wire says Al Jazeera America will...  Middle-east   \n",
       "23480  21st Century Wire says As 21WIRE predicted in ...  Middle-east   \n",
       "\n",
       "                    date label  \n",
       "0      December 31, 2017  Fake  \n",
       "1      December 31, 2017  Fake  \n",
       "2      December 30, 2017  Fake  \n",
       "3      December 29, 2017  Fake  \n",
       "4      December 25, 2017  Fake  \n",
       "...                  ...   ...  \n",
       "23476   January 16, 2016  Fake  \n",
       "23477   January 16, 2016  Fake  \n",
       "23478   January 15, 2016  Fake  \n",
       "23479   January 14, 2016  Fake  \n",
       "23480   January 12, 2016  Fake  \n",
       "\n",
       "[23481 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First change the alllabels to fake as we know this is the fake dataset\n",
    "df_fake['label']='Fake'\n",
    "df_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c57ddfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function using our classifer\n",
    "# Takes an article text and returns \"Real\" or \"Fake\"\n",
    "def NB_detectFake(text): # New text is raw text\n",
    "    #test gets the tokenized vectors from our feature extractor\n",
    "    test=tfidf_vectorizer.transform([text])\n",
    "    # pred is the prediction from our Naive Bayes classifer\n",
    "    pred=NB_classifier.predict(test)\n",
    "    return pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea00698",
   "metadata": {},
   "source": [
    "Now lets iterate through the fake csv file and see how we did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f374987c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent fake news detected:7.62%\n"
     ]
    }
   ],
   "source": [
    "# Accuracy; how many fake news article were correctly predicted to be fake\n",
    "sum = 0 # Set the sum to 0\n",
    "for i in range(len(df_fake)): # Iterate over the len of the dataset\n",
    "    if(NB_detectFake(df_fake['text'][i])=='Fake'): # if we return Fake for a text, increase sum\n",
    "        sum = sum + 1\n",
    "        \n",
    "percent = round(sum / len(df_fake) * 100, 2) # Divide the number of fakes we found by the number of text\n",
    "print(f'Percent fake news detected:{percent}%') # Print out the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6cc794",
   "metadata": {},
   "source": [
    "Oof, not nearly as good as the metrics would have us believe eh? Far, far below the above accuracy. It seems Naive Bayes doesn't play well in our field test. Lets try out the Passive Agressive Classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c37d3e",
   "metadata": {},
   "source": [
    "## Performance of the Passive Agressive Classifier\n",
    "Now let us try the same, but this time trying with a different classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1dce36ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>PassiveAggressiveClassifier(max_iter=50)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PassiveAggressiveClassifier</label><div class=\"sk-toggleable__content\"><pre>PassiveAggressiveClassifier(max_iter=50)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "PassiveAggressiveClassifier(max_iter=50)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import and set classifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "pac_classifier = PassiveAggressiveClassifier(max_iter=50) # the default is 50\n",
    "pac_classifier.fit(tfidf_vec_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3f0ffba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pac Accuracy: 96.58%\n"
     ]
    }
   ],
   "source": [
    "# Set the prediction, score and accuracy\n",
    "pac_y_pred = pac_classifier.predict(tfidf_vec_test)\n",
    "pac_score = accuracy_score(y_test, pac_y_pred)\n",
    "pac_accuracy = round(pac_score*100,2)\n",
    "print(f'Pac Accuracy: {pac_accuracy}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c786c6",
   "metadata": {},
   "source": [
    "96%! Wow, a bit above our bench mark again, but lets take a closer look and then give it a field test. Remeber how poorly the above did?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7dd68a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.96      0.96      0.96      1580\n",
      "        Real       0.97      0.97      0.97      2077\n",
      "\n",
      "    accuracy                           0.97      3657\n",
      "   macro avg       0.97      0.96      0.97      3657\n",
      "weighted avg       0.97      0.97      0.97      3657\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the report\n",
    "print(classification_report(y_test, pac_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43e0967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above, returns Real or Fake, but uses t he Passive Agressive Classifier\n",
    "def PAC_detectFake(text): # New text is raw text\n",
    "    #test gets the tokenized vectors from our feature extractor\n",
    "    test=tfidf_vectorizer.transform([text])\n",
    "    # pred is the prediction from our Passive Agressive classifer\n",
    "    pred=pac_classifier.predict(test)\n",
    "    return pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ef2d536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent fake news detected:66.35%\n"
     ]
    }
   ],
   "source": [
    "# Accuracy; how many fake news article were correctly predicted to be fake\n",
    "sum = 0 # Set the sum to 0\n",
    "for i in range(len(df_fake)): # Iterate over the len of the dataset\n",
    "    if(PAC_detectFake(df_fake['text'][i])=='Fake'): # if we return Fake for a text, increase sum\n",
    "        sum = sum + 1\n",
    "        \n",
    "percent = round(sum / len(df_fake) * 100, 2) # Divide the number of fakes we found by the number of text\n",
    "print(f'Percent fake news detected:{percent}%') # Print out the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8624a84e",
   "metadata": {},
   "source": [
    "A bit underwhelming, no? Only 66% in our field test. It seems it performs much better on the trained data rather than the other data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e83f460",
   "metadata": {},
   "source": [
    "# 3 - Conclusions / Outcome\n",
    "## Performance\n",
    "Accroding to the above we can see that the accuracy for the naivebayes MultinomialNB was around 86%, however when we tested it to find the fakes and true news when given only fake and real news it did terrible, with only 7% for the fake news. The Passive Aggressive Classifer, on the other hand had a sligtly better score at 96% according to the report, but when we tested it on the Fake only stories the performance was again, worse. But it is important to not it still managed to find nearly 67% of the fake news. \n",
    "This coould be due to the data in the \"field test\" ( a.k.a fake.csv ) being different in structure than the training data as theu came from different sources. \n",
    "\n",
    "## Summary\n",
    "Clearly detecting fake news is something that perhaps a simple classifier may not be well suited to. Even when the results of the testing came back looking very good, when tested in our psuedo \"real world\" test, the accuracy quickly dropped off.\n",
    "#### Areas for improvement\n",
    "In this paper, I looked at only the body of text itself, however the source of the data is also a very strong indicator of Real or Fake news, as is the author themselves. Given those data points were omitted in an effort to find any common patterns in just the body of the text, perhaps the peformance would be greatly boosted by those. However, it is important to note that new \"Fake\" sources are popping up all the time, and that could ultimately lead to worse performance as new unkown could skew the results(i.e a new source might automatically seem more \"Real\" due to the fitting).\n",
    "#### Further devlopment\n",
    "In this case I actually feel that more data would be the most useful, we had only a total of 20,000 data points to train on and that may have been an issue when presented with our challenge data. As a even further task, perhaps applying deep learning would yield even more promising resluts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
